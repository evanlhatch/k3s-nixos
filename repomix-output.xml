This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
disko-configs/
  generic-disko-layout.nix
  hetzner-disko-layout.nix
hardware info/
  hetzner/
    cpx21/
      hardware-configuration.nix
    cpx31/
      hardware-configuration.nix
  selfhost/
    thinkpad-x1-extreme/
      hardware-configuration.nix
  hetzner-hardware.nix
k3s-cluster/
  locations/
    hetzner.nix
    local.nix
  modules/
    infisical-agent.nix
    netdata.nix
    tailscale.nix
  profiles/
    base-server.nix
  roles/
    k3s-control.nix
    k3s-worker.nix
  common.nix
.env.example
.envrc
.gitignore
.infisical.json
devenv.nix
devenv.yaml
dummy-hardware-config.nix
flake.nix
go.mod
magefile.go
README.md
rehtsetr
sops.secrets.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="disko-configs/hetzner-disko-layout.nix">
# !k3s-nixos-configs/hardware/hetzner/cpx21/disko-layout.nix
{ lib, pkgs, ... }:
{
  disko.devices = {
    disk = {
      mainDisk = {
        device = "/dev/sda"; # Corrected to /dev/sda for Hetzner CPX21
        type = "disk";
        content = {
          type = "gpt";
          partitions = {
            # BIOS Boot Partition (for legacy boot compatibility)
            biosboot = {
              name = "BIOSBOOT";
              size = "1M";
              type = "EF02"; # GRUB BIOS Boot partition type
            };
            # EFI System Partition (ESP)
            esp = {
              name = "ESP";
              size = "1G"; # Increased from 512M to 1G as requested
              type = "EF00"; # EFI System Partition type
              content = {
                type = "filesystem";
                format = "vfat"; # FAT32 for ESP
                mountpoint = "/boot";
              };
            };
            # LVM Physical Volume
            lvm_pv = {
              name = "LVM_PV_MAIN";
              size = "100%"; # Use all remaining space
              content = {
                type = "lvm_pv";
                vg = "vg_main"; # Assign this PV to vg_main
              };
            };
          };
        };
      };
    };
    lvm_vg = {
      vg_main = {
        # Define the Volume Group
        type = "lvm_vg";
        lvs = {
          # Logical Volume for Root Filesystem
          root = {
            name = "lv_root";
            size = "100%FREE"; # All free space in VG
            content = {
              type = "filesystem";
              format = "ext4";
              mountpoint = "/";
              mountOptions = [
                "defaults"
                "discard"
              ]; # Add discard for SSDs if appropriate
            };
          };
          # Logical Volume for Swap
          swap = {
            name = "lv_swap";
            size = "4G"; # 4GB Swap LV
            content = {
              type = "swap";
            };
          };
        };
      };
    };
  };
}
</file>

<file path="hardware info/hetzner/cpx21/hardware-configuration.nix">
{ modulesPath, lib, ... }:
{
  imports = [
    (modulesPath + "/profiles/qemu-guest.nix")
  ];

  # == Settings Previously in configuration.nix ==
  # Basic hardware configuration
  boot.initrd.availableKernelModules = [
    "ata_piix"
    "uhci_hcd"
    "xen_blkfront"
    "vmw_pvscsi"
    "virtio_blk"
    "virtio_pci"
  ];
  boot.initrd.kernelModules = [
    "nvme"
    "virtio_blk"
  ];

  # Hetzner-specific settings
  services.qemuGuest.enable = true;

  # CPX21 specs: 3 cores, 4GB RAM, 80GB disk
  nix.settings.max-jobs = lib.mkDefault 3;
  # == End Settings Previously in configuration.nix ==

  # Boot loader configuration
  boot.loader.grub = {
    # Explicitly set devices to ensure GRUB is installed to the MBR of /dev/sda
    devices = [ "/dev/sda" ];
    efiSupport = true;
    efiInstallAsRemovable = true;
  };

  # Note: Don't set fileSystems here as disko will handle that
  # The previous setting is removed as it conflicts with disko:
  # fileSystems."/" = { device = "/dev/sda1"; fsType = "ext4"; };

  # CPX21 specs: 3 cores, 4GB RAM, 80GB disk
}
</file>

<file path="hardware info/hetzner/cpx31/hardware-configuration.nix">
{ modulesPath, ... }:
{
  imports = [ (modulesPath + "/profiles/qemu-guest.nix") ];
  boot.loader.grub = {
    enable = true;
    devices = [ "/dev/sda" ];
    efiSupport = false;
  };
  boot.initrd.availableKernelModules = [
    "ata_piix"
    "uhci_hcd"
    "xen_blkfront"
    "vmw_pvscsi"
  ];
  boot.initrd.kernelModules = [ "nvme" ];
  # Commented out to avoid conflicts with disko
  # fileSystems."/" = {
  #   device = "/dev/sda1";
  #   fsType = "ext4";
  # };

  # CPX31 specs: 4 cores, 8GB RAM, 160GB disk
}
</file>

<file path="hardware info/selfhost/thinkpad-x1-extreme/hardware-configuration.nix">
# Do not modify this file!  It was generated by â€˜nixos-generate-configâ€™
# and may be overwritten by future invocations.  Please make changes
# to /etc/nixos/configuration.nix instead.
{
  config,
  lib,
  pkgs,
  modulesPath,
  ...
}:

{
  imports = [
    (modulesPath + "/installer/scan/not-detected.nix")
  ];

  boot.initrd.availableKernelModules = [
    "xhci_pci"
    "nvme"
    "usb_storage"
    "sd_mod"
  ];
  boot.initrd.kernelModules = [ ];
  boot.kernelModules = [ "kvm-intel" ];
  boot.extraModulePackages = [ ];

  fileSystems."/" = {
    device = "/dev/disk/by-uuid/1e9bf502-7ca7-4c62-ab11-755a31c0724a";
    fsType = "ext4";
  };

  fileSystems."/boot" = {
    device = "/dev/disk/by-uuid/12CE-A600";
    fsType = "vfat";
    options = [
      "fmask=0022"
      "dmask=0022"
    ];
  };

  swapDevices = [ ];

  # Enables DHCP on each ethernet and wireless interface. In case of scripted networking
  # (the default) this is the recommended approach. When using systemd-networkd it's
  # still possible to use this option, but it's recommended to use it in conjunction
  # with explicit per-interface declarations with `networking.interfaces.<interface>.useDHCP`.
  networking.useDHCP = lib.mkDefault true;
  # networking.interfaces.enp0s31f6.useDHCP = lib.mkDefault true;
  # networking.interfaces.enp58s0u1.useDHCP = lib.mkDefault true;

  nixpkgs.hostPlatform = lib.mkDefault "x86_64-linux";
  hardware.cpu.intel.updateMicrocode = lib.mkDefault config.hardware.enableRedistributableFirmware;
}
</file>

<file path="hardware info/hetzner-hardware.nix">
{
  config,
  lib,
  pkgs,
  modulesPath,
  ...
}:

{
  # Hetzner Cloud specific hardware configuration

  # Import qemu-guest profile
  imports = [
    (modulesPath + "/profiles/qemu-guest.nix")
  ];

  # Hetzner Cloud specific kernel modules
  boot.kernelModules = [
    "virtio_pci"
    "virtio_scsi"
    "nvme"
    "ata_piix"
    "uhci_hcd"
  ];

  # Hetzner Cloud specific boot settings
  boot.loader.grub = {
    enable = true;
    device = "/dev/sda";
    efiSupport = false;
    efiInstallAsRemovable = false;
  };

  # Hetzner Cloud specific filesystem settings
  # Commented out to avoid conflicts with disko
  # fileSystems."/" = lib.mkDefault {
  #   device = "/dev/sda1";
  #   fsType = "ext4";
  # };

  # Hetzner Cloud specific swap settings
  swapDevices = [ ];

  # Enable qemu-guest-agent for Hetzner Cloud
  services.qemuGuest.enable = true;

  # Disable power management
  powerManagement.enable = false;

  # Disable X11
  services.xserver.enable = false;

  # Disable bluetooth
  hardware.bluetooth.enable = false;
}
</file>

<file path="k3s-cluster/locations/hetzner.nix">
# ./k3s-cluster/locations/hetzner.nix
{
  config,
  lib,
  pkgs,
  specialArgs,
  ...
}:
{
  # Enable qemuGuest service directly instead of importing the module
  imports = [ ];

  boot.initrd.availableKernelModules = [
    "virtio_pci"
    "virtio_blk"
    "nvme"
    "xhci_pci"
    "sr_mod"
    "ata_piix"
    "uhci_hcd"
  ];
  boot.kernelModules = [ "virtio_net" ]; # Important for Hetzner networking

  # systemd-networkd is enabled by profiles/base-server.nix.
  # This configures the standard Hetzner interfaces.
  systemd.network.networks = {
    "10-public" = {
      matchConfig.Name = specialArgs.hetznerPublicInterface or "eth0";
      networkConfig = {
        DHCP = "ipv4";
        IPv6AcceptRA = true;
      };
    };
    "20-private" =
      lib.mkIf (specialArgs.hetznerPrivateInterface != null && specialArgs.hetznerPrivateInterface != "")
        {
          matchConfig.Name = specialArgs.hetznerPrivateInterface;
          networkConfig = {
            DHCP = "ipv4";
          }; # Usually gets IP from Hetzner private net DHCP
          linkConfig.RequiredForOnline = "no";
        };
  };

  # DO NOT force fileSystems."/" or boot.loader here.
  # Let it be defined by the auto-generated hardware-configuration.nix (for nixos-everywhere)
  # or by a disko configuration (for image builds).

  services.cloud-init = {
    enable = true;
    # Let systemd-networkd handle actual network config based on definitions above.
    # Cloud-init primarily for user-data (ssh keys, role file).
    network.enable = false;
  };
  services.qemuGuest.enable = true;
  time.timeZone = lib.mkDefault "Etc/UTC"; # Servers should be UTC
}
</file>

<file path="k3s-cluster/modules/infisical-agent.nix">
# k3s-cluster/modules/infisical-agent.nix
{
  config,
  lib,
  pkgs,
  specialArgs ? { },
  ...
}:

let
  # Get the Infisical address from sops-nix decrypted file content
  # Ensure 'infisical_address' is defined in sops.secrets in commonSopsModule
  infisicalAddress = builtins.readFile (config.sops.secrets.infisical_address.path);

  # Get paths to client ID and secret files, also managed by sops-nix
  # Ensure 'infisical_client_id' and 'infisical_client_secret' are defined in sops.secrets
  infisicalClientIdPath = config.sops.secrets.infisical_client_id.path;
  infisicalClientSecretPath = config.sops.secrets.infisical_client_secret.path;

  # Check if the agent should be enabled (passed via specialArgs from flake.nix)
  enableAgent = specialArgs.enableInfisicalAgent or false;
in
lib.mkIf enableAgent {
  environment.systemPackages = [ pkgs.infisical ];

  # sops-nix creates the secret files. We just need to ensure the agent's config dir exists.
  systemd.tmpfiles.rules = [
    "d /etc/infisical 0750 root root - -" # For agent.yaml
    "d /run/infisical-secrets 0750 root root - -" # For rendered secrets by agent
  ];

  environment.etc."infisical/agent.yaml" = {
    mode = "0400"; # Readable only by root
    text = ''
      infisical:
        address: "${infisicalAddress}" # Content read from sops-decrypted file
      auth:
        type: "universal-auth"
        config:
          client-id_file: ${infisicalClientIdPath}        # Path to sops-decrypted file
          client-secret_file: ${infisicalClientSecretPath} # Path to sops-decrypted file
          # remove_client_secret_on_read: true # Consider for enhanced security

      templates:
        - destination_path: /run/infisical-secrets/k3s_token
          template_content: |
            {{ secret "/k3s-bootstrap" "K3S_TOKEN" }}
          config:
            permissions: "0400"
        - destination_path: /run/infisical-secrets/tailscale_join_key
          template_content: |
            {{ secret "/k3s-bootstrap" "TAILSCALE_AUTH_KEY" }}
          config:
            permissions: "0400"
    '';
  };

  systemd.services.infisical-agent = {
    description = "Infisical Agent Daemon";
    wantedBy = [ "multi-user.target" ];
    after = [
      "network-online.target"
      "sops-secrets.service" # Ensure sops secrets (like client_id file) are ready
    ];
    wants = [
      "network-online.target"
      "sops-secrets.service"
    ];
    before = [
      "k3s.service"
      "k3s-agent.service"
    ];
    serviceConfig = {
      Type = "simple";
      ExecStart = ''
        ${pkgs.infisical}/bin/infisical-agent --config /etc/infisical/agent.yaml daemon start
      '';
      Restart = "on-failure";
      RestartSec = "10s";
      User = "root"; # Assuming agent needs root to write to /run/infisical-secrets
    };
  };
}
</file>

<file path="k3s-cluster/modules/netdata.nix">
# /home/evan/2_Dev/2.1_Homelab/!k3s-nixos-configs/k3s-cluster/modules/netdata.nix
{
  config,
  lib,
  pkgs,
  specialArgs ? { },
  ...
}:

let
  # --- Configurable Access Control ---
  # Define who can access the Netdata dashboard.
  # Defaults to a restrictive set: localhost and common private/Tailscale ranges.
  # This can be overridden by passing `netdataAllowedSources = [ ... ]` in `specialArgs`
  # when this module is imported by a NixOS configuration in your Flake.
  defaultAllowedSources = [
    "localhost" # Always allow local access for diagnostics
    "10.0.0.0/8" # Common private RFC1918 range (covers Hetzner private net)
    "172.16.0.0/12" # Common private RFC1918 range
    "192.168.0.0/16" # Common private RFC1918 range
    "100.64.0.0/10" # Tailscale CGNAT range
    "fd00::/8" # Tailscale IPv6 ULA range
    # Example: Add a specific admin IP if needed and known
    # (if specialArgs.adminPublicIp != null then specialArgs.adminPublicIp else "127.0.0.1") # Safely default if not set
  ];

  # Use provided list or fall back to defaults
  allowedSourcesList = specialArgs.netdataAllowedSources or defaultAllowedSources;

  # Convert the list to a space-separated string required by Netdata config
  allowedSourcesConfigString = lib.concatStringsSep " " allowedSourcesList;

  # --- Configurable Performance/Storage ---
  updateInterval = specialArgs.netdataUpdateInterval or 2; # Seconds, increased from 1 to slightly reduce load
  pageCacheSizeMB = specialArgs.netdataPageCacheSizeMb or 32; # MB
  dbengineDiskSpaceMB = specialArgs.netdataDbengineDiskSpaceMb or 256; # MB
  historySeconds = specialArgs.netdataHistorySeconds or 7200; # Default 2 hours of metrics (was 3600)

in
{
  services.netdata = {
    enable = true;
    package = pkgs.netdata; # Explicitly use the version from nixpkgs

    # Core Netdata configuration
    config = {
      global = {
        "update every" = updateInterval;
        "memory mode" = "dbengine"; # Efficient storage, good for servers
        "page cache size" = pageCacheSizeMB;
        "dbengine multihost disk space" = dbengineDiskSpaceMB;
        "history" = historySeconds; # How long to keep metrics
        # Consider error log settings for production
        # "error log" = "/var/log/netdata/error.log";
        # "debug log" = "/var/log/netdata/debug.log"; # Usually none for production
      };

      web = {
        "default port" = 19999;
        # Secure access to the dashboard
        "allow connections from" = allowedSourcesConfigString;
        "allow dashboard from" = allowedSourcesConfigString;
        # Further restrict access to the config file itself
        "allow netdata.conf from" = "localhost unixdomain";
        # Consider disabling web stats if not needed
        # "web server statistics" = "no";
      };

      plugins = {
        # Core system monitoring plugins (generally useful)
        "apps" = true; # Per-application resource usage
        "cgroups" = true; # Essential for container monitoring (K3s uses containerd)
        "diskspace" = true; # Filesystem usage
        "proc" = true; # Kernel (/proc) based metrics: CPU, net, disk I/O, etc.
        "python.d" = true; # Enable python plugin engine for broader capabilities
        "go.d" = true; # Enable go plugin engine (e.g., for Kubernetes, Docker collectors if present)

        # Plugins to consider disabling on a lean server if not explicitly needed:
        "tc" = false; # Traffic Control, can be resource-intensive/noisy
        # "charts.d" = false; # Older shell-based collectors, usually covered by others
        # "fping" = false;    # If external pinging isn't critical from this node
        # "slabinfo" = false; # Kernel slab allocator, can be verbose
      };

      # Example: Specific collector configuration (Netdata usually auto-detects well)
      # This section allows fine-tuning if auto-detection isn't sufficient or if you
      # want to ensure specific collectors run or are disabled.
      # "plugin:go.d:kubernetes" = { # If go.d plugin has a kubernetes job
      #   "enabled" = "yes";
      #   # "kubelet_url" = "http://127.0.0.1:10255"; # If Kubelet metrics are available
      # };
      # "plugin:cgroups:containerd" = { # If specific containerd settings are needed
      #   "enabled" = "yes";
      # };
    };

    # Python plugin configuration
    python = {
      enable = true; # Enables the python.d.plugin itself
      # `recommendedPythonPackages = true;` installs a broad set of Python libraries
      # for many potential plugins. For a production server, you might want to be more
      # selective if you know which specific python.d plugins you'll use (if any)
      # to keep the closure size smaller.
      # If no specific python.d plugins are planned beyond what Netdata auto-enables,
      # you could consider setting recommendedPythonPackages = false and only add
      # specific dependencies if a python.d chart complains.
      recommendedPythonPackages = true; # Keep for now for broad out-of-the-box data
      # extraPackages = ps: [ ps.psutil ]; # Example if a specific plugin needed psutil
    };

    # Disable sending anonymous usage statistics to Netdata
    enableAnalyticsReporting = false;
  };

  # Ensure the NixOS host firewall allows access to Netdata's port
  # This should be from the same sources as defined in `allowedSourcesConfigString`.
  # However, NixOS firewall rules are simpler (just port and optionally interface).
  # The actual IP-based restriction is best handled by Netdata itself ("allow connections from")
  # and the cloud firewall (Hetzner Firewall).
  networking.firewall.allowedTCPPorts = [ 19999 ];
}
</file>

<file path="k3s-cluster/modules/tailscale.nix">
# ./k3s-cluster/modules/tailscale.nix
{
  config,
  lib,
  pkgs,
  specialArgs ? { },
  ...
}:
# This module ensures the Tailscale package is installed and basic firewall rules are set.
# It does NOT enable or configure the tailscaled service directly if K3s is managing
# the Tailscale connection via its --vpn-auth mechanism.
{
  environment.systemPackages = [ pkgs.tailscale ];

  networking.firewall = {
    # If NixOS firewall is active, allow Tailscale's default ports.
    # Note: config.services.tailscale.port is only defined if services.tailscale.enable = true.
    # Hardcoding is safer if services.tailscale is not explicitly enabled by this module.
    allowedUDPPorts = [ 41641 ]; # Default Tailscale port
    # allowedTCPPorts = [ 443 ]; # For DERP over HTTPS if UDP blocked, less common for servers
    trustedInterfaces = [ "tailscale0" ]; # Trust traffic from Tailscale interface
  };

  # If you need to manage tailscaled service independently (e.g., for nodes NOT using K3s vpn-auth):
  # services.tailscale.enable = lib.mkIf (specialArgs.nodeSecretsProvider == "sops" && specialArgs.enableStandaloneTailscale == true) true;
  # services.tailscale.authKeyFile = lib.mkIf (specialArgs.nodeSecretsProvider == "sops" && specialArgs.enableStandaloneTailscale == true) config.sops.secrets.tailscale_authkey.path;
}
</file>

<file path="k3s-cluster/roles/k3s-control.nix">
# ./k3s-cluster/roles/k3s-control.nix
{
  config,
  lib,
  pkgs,
  specialArgs,
  ...
}:

let
  # Define paths for K3s token and Tailscale auth key using sops-nix
  k3sTokenFile = config.sops.secrets.k3s_token.path;
  tailscaleAuthKeyFile = config.sops.secrets.tailscale_auth_key.path;

  # Control Plane IP should be its own Tailscale IP or a stable private IP known to other nodes.
  # specialArgs.k3sControlPlaneAddr is the IP other nodes use to connect.
  # For --node-ip, we use its own private IP or Tailscale IP.
  nodeIp = specialArgs.nodeIPAddress or specialArgs.k3sControlPlaneAddr; # nodeIPAddress should be specific to this node

in
{
  # Define the K3s server systemd service
  systemd.services.k3s = {
    description = "Lightweight Kubernetes (Server / Control Plane)";
    wantedBy = [ "multi-user.target" ];
    # Must start after network is fully up and secrets (if any) are available
    after = [
      "network-online.target"
      "sops-nix.service"
    ];
    wants = [
      "network-online.target"
      "sops-nix.service"
    ];
    serviceConfig = {
      Type = "notify";
      ExecStart = "${pkgs.k3s}/bin/k3s server"; # Base command
      KillMode = "process";
      Delegate = true;
      LimitNOFILE = 1048576;
      LimitNPROC = "infinity";
      LimitCORE = "infinity";
      TasksMax = "infinity";
      TimeoutStartSec = 0; # K3s handles its own startup timeout logic effectively
      Restart = "always";
      RestartSec = "10s"; # Increased restart delay
    };
  };

  # Configure K3s service
  services.k3s = {
    # This 'enable' flag is typically false for generic images using k3s-role-selector.
    # For a dedicated control-plane node definition in the Flake, set it to true.
    enable = specialArgs.enableK3sServiceByDefault or false;
    role = "server";
    tokenFile = k3sTokenFile; # Use the conditionally defined path

    extraFlags = toString (
      [
        "--node-ip=${nodeIp}" # Use this node's own IP (private or Tailscale)
        "--advertise-address=${nodeIp}" # Advertise this node's IP
        "--bind-address=0.0.0.0" # Listen on all interfaces
        "--kubelet-arg=cloud-provider=external" # For Hetzner CCM
        "--disable-cloud-controller" # We install CCM separately via Flux
        "--disable=servicelb,traefik,local-storage" # Disable built-ins we replace
        # Tailscale CNI integration flags
        "--flannel-backend=none"
        "--disable-network-policy" # If using Tailscale ACLs for network policy
        "--node-external-ip=$(tailscale ip -4)" # Use Tailscale IP for node's external IP concept in K8s
        "--vpn-auth-file=${tailscaleAuthKeyFile}"
        "--vpn-auth-name=k3s-${specialArgs.hostname}" # Unique name for Tailscale device
        # Example: add extra args for --vpn-auth for tags
        # "--vpn-auth-extra-args=--advertise-tags=tag:k3s-control,tag:k3s-cluster-${config.networking.hostName}"
      ]
      ++ (lib.optional (specialArgs.isFirstControlPlane == true) "--cluster-init") # Only for the very first control-plane node
      # Example: Pass datastore endpoint if using external DB for HA
      # ++ (lib.optionals (specialArgs.k3sDatastoreEndpoint != null) [ "--datastore-endpoint=${specialArgs.k3sDatastoreEndpoint}" ])
    );

    # Example config.yaml if needed, though flags are often sufficient for K3s
    # configYAML = pkgs.lib.generators.toYAML {} {
    #   "cluster-cidr" = "10.42.0.0/16"; # Default K3s
    #   "service-cidr" = "10.43.0.0/16"; # Default K3s
    #   # Add other static config.yaml settings here
    # };
  };

  # Firewall rules specific to control-plane
  networking.firewall.allowedTCPPorts = [
    6443 # Kubernetes API Server
    2379 # etcd client (if embedded etcd, for HA)
    2380 # etcd peer (if embedded etcd, for HA)
    10250 # Kubelet (if metrics-server or other components need to reach it directly on control plane)
  ];
  # Tailscale UDP port (41641) should be opened by the tailscale module or base firewall config

  # Essential Kubernetes client tools
  environment.systemPackages = with pkgs; [
    kubectl
    kubernetes-helm
    fluxcd # Flux CLI for interacting with the cluster
  ];
}
</file>

<file path="k3s-cluster/roles/k3s-worker.nix">
# ./roles/k3s-worker.nix (Illustrative, apply similar logic to k3s-control.nix)
{
  config,
  lib,
  pkgs,
  specialArgs,
  ...
}:
let
  # isInfisical = specialArgs.nodeSecretsProvider == "infisical"; # Removed

  k3sTokenFile = "/run/infisical-secrets/k3s_token"; # Changed
  # if isInfisical then "/run/infisical-secrets/k3s_token" else config.sops.secrets.k3s_token.path; # Assumes 'k3s_token' is the sops secret name

  tailscaleAuthKeyFile = "/run/infisical-secrets/tailscale_join_key"; # Changed
  # if isInfisical then
  #   "/run/infisical-secrets/tailscale_join_key"
  # else
  #   config.sops.secrets.tailscale_authkey.path; # Assumes 'tailscale_authkey' is sops name
in
{
  # K3s Agent Service Definition (should be in systemd.services for clarity)
  systemd.services.k3s-agent = {
    description = "Lightweight Kubernetes (Agent)";
    wantedBy = [ "multi-user.target" ];
    after = [
      "network-online.target"
      "infisical-agent.service"
    ];
    wants = [
      "network-online.target"
      "infisical-agent.service"
    ];
    serviceConfig = {
      Type = "notify";
      ExecStart = "${pkgs.k3s}/bin/k3s agent";
      # ... other serviceConfig options from your plan ...
      Restart = "always";
      RestartSec = "10s";
    };
  };

  services.k3s = {
    # This enable flag is controlled by the k3s-role-selector service in generic images,
    # or can be set to `true` directly in a specific Flake nixosConfiguration for a dedicated node.
    enable = false;
    role = "agent";
    # Use the control plane address passed via specialArgs
    serverAddr = "https://${specialArgs.k3sControlPlaneAddr}:6443";
    tokenFile = k3sTokenFile; # Dynamically set path
    extraFlags = toString ([
      "--kubelet-arg=cloud-provider=external" # For Hetzner CCM
      # K3s with Tailscale CNI integration flags
      "--flannel-backend=none"
      "--disable-network-policy" # Tailscale ACLs manage policy
      # Following flags require Tailscale to be up and accessible.
      # K3s --vpn-auth should handle bringing Tailscale up.
      "--node-external-ip=$(tailscale ip -4)" # Use node's Tailscale IP
      "--vpn-auth-file=${tailscaleAuthKeyFile}" # Use the key provided by Infisical/Sops
      "--vpn-auth-name=k3s-${specialArgs.hostname}" # Unique name for the Tailscale VPN device
      # Add other node labels or taints as needed, possibly from specialArgs
      # e.g., "--node-label=role=${specialArgs.role}"
    ]
    # Example of adding custom taints from specialArgs
    # ++ (lib.mapAttrsToList (name: value: "--node-taint=${name}=${value}:NoSchedule") (specialArgs.nodeTaints or {}))
    );
  };

  # Worker-specific firewall rules (K3s CNI might manage some itself)
  networking.firewall.allowedTCPPorts = [ 10250 ]; # Kubelet API
  # Tailscale UDP port is handled by the tailscale module or base firewall.

  environment.systemPackages = with pkgs; [ kubectl ]; # For debugging
}
</file>

<file path=".env.example">
# Hetzner Cloud API token
HCLOUD_TOKEN=your_hetzner_cloud_token_here

# SSH key name in Hetzner Cloud
HETZNER_SSH_KEY_NAME="blade-nixos SSH Key"

# Network configuration
PRIVATE_NETWORK_NAME=k3s-net
FIREWALL_NAME=k3s-fw
PLACEMENT_GROUP_NAME=k3s-placement-group
K3S_CLUSTER_NAME=k3s-cluster
HETZNER_LOCATION=ash
HETZNER_NETWORK_ZONE=us-east
HETZNER_IMAGE_NAME=debian-12
CONTROL_PLANE_VM_TYPE=cpx21

# Admin configuration
ADMIN_PUBLIC_IP=0.0.0.0/0
ADMIN_USERNAME=nixos

# SSH key path
MAGE_SSH_KEY=~/.ssh/id_rsa

# Age private key for secrets
AGE_PRIVATE_KEY=your_age_private_key_here

# Default IPv4 setting
HETZNER_DEFAULT_ENABLE_IPV4=true

# Node configuration
NODE_HOSTNAME=your_node_hostname_here
IS_FIRST_CONTROL_PLANE=true
</file>

<file path=".infisical.json">
{
    "workspaceId": "2f738ae8-2dc6-498d-97ab-7e7792efd657",
    "defaultEnvironment": "",
    "gitBranchToEnvironmentMapping": null
}
</file>

<file path="devenv.yaml">
inputs:
  nixpkgs:
    url: github:NixOS/nixpkgs/nixos-unstable
</file>

<file path="dummy-hardware-config.nix">
# ./dummy-hardware-config.nix
# A minimal, valid NixOS module to satisfy nix flake check when
# /etc/nixos/hardware-configuration.nix is not available during local evaluation.
{
  config,
  pkgs,
  lib,
  ...
}:

{
  imports = [
    # You can try to import the generic "not-detected.nix" if you know its store path
    # or have a copy, but often a minimal stub like this is sufficient for checks.
    # Example: (pkgs.path + "/nixos/modules/installer/scan/not-detected.nix") # Path may vary
  ];

  # Provide minimal valid settings to make this a functional module for evaluation.
  # These settings are primarily for allowing `nix flake check` to pass purely
  # and will NOT be used for actual deployments with nixos-anywhere if you are using "Option A"
  # (where the installer generates the real /etc/nixos/hardware-configuration.nix).

  boot.initrd.availableKernelModules = [
    "ahci" # Common SATA controller
    "sd_mod" # SCSI disk support (often needed for virtio-scsi)
    "nvme" # NVMe disk support
    "virtio_blk" # VirtIO block device (common in VMs)
    "usb_storage" # For USB devices
    "uas" # USB Attached SCSI
    # Add other common filesystem or bus modules if your checks go deep
    "ext4"
    "vfat"
  ];

  fileSystems."/" = {
    device = "/dev/disk/by-label/NIXOS_DUMMY_ROOT"; # A non-existent device for dummy purposes
    fsType = "ext4";
  };

  # A dummy /boot is often needed if a bootloader is enabled/checked
  fileSystems."/boot" =
    lib.mkIf (config.boot.loader.grub.enable || config.boot.loader.systemd-boot.enable)
      {
        device = "/dev/disk/by-label/NIXOS_DUMMY_BOOT"; # Dummy boot partition
        fsType = "vfat";
      };

  swapDevices = [
    # { device = "/dev/disk/by-label/NIXOS_DUMMY_SWAP"; } # Dummy swap
  ];

  # Ensure a bootloader is configured if your dummy config needs to evaluate that far.
  # For basic flake checks, this might not be strictly necessary if no other module
  # asserts on bootloader settings.
  boot.loader.grub.enable = lib.mkDefault true; # Or systemd-boot
  boot.loader.grub.device = lib.mkDefault "/dev/sda"; # Dummy device for grub install (won't actually run)

  # This is important if your other modules try to reference networking.hostName during evaluation.
  # It will be overridden by the actual hostname from specialArgs in a real build.
  networking.hostName = lib.mkDefault "dummycheckhost";

  # Set a state version for the dummy config as well.
  system.stateVersion = lib.mkDefault config.specialArgs.nixosStateVersion;
}
</file>

<file path="go.mod">
module k3s-nixos-configs

go 1.21

require (
	github.com/joho/godotenv v1.5.1
	github.com/magefile/mage v1.15.0
)
</file>

<file path="README.md">
# K3s NixOS Configs

This repository contains NixOS configurations for a K3s Kubernetes cluster.

## Prerequisites

- [Nix](https://nixos.org/download.html) with flakes enabled
- [mage](https://magefile.org/) - Available in nixpkgs

## Setup

1. Clone the repository
2. Copy the `.env.example` file to `.env` and fill in the required values

## Available Commands

Run `mage -l` to see all available commands:

```
Targets:
  checkFlake*                runs `nix flake check` to validate the flake.
  deleteAndRedeployServer    deletes an existing server, recreates it, and then deploys NixOS to it.
  deploy                     deploys a given NixOS configuration to its target host using deploy-rs.
  rebuild                    performs a nixos-rebuild switch on a target node.
  recreateNode               redeploys a node using nixos-anywhere and fetches its K3s config.
  recreateServer             recreates a Hetzner Cloud server with the specified properties.
  showFlake                  runs `nix flake show`.
  updateFlake                runs `nix flake update` to update all flake inputs.

* default target
```

### Common Usage

- `mage deploy <flakeConfigName>`: Deploy a NixOS configuration using deploy-rs
- `mage recreateNode <flakeConfigName>`: Deploy a NixOS configuration using nixos-anywhere
- `mage recreateServer <serverName> <ipv4Enabled>`: Recreate a Hetzner Cloud server
- `mage deleteAndRedeployServer <serverName> <flakeConfigName> <ipv4Enabled>`: Delete and redeploy a server

## Optional Development Environment

This project also includes a [devenv](https://devenv.sh/) configuration for those who prefer a more isolated development environment.

### Setup with devenv

1. Install [direnv](https://direnv.net/docs/installation.html) and [devenv](https://devenv.sh/getting-started/)
2. Run `direnv allow` to automatically load the devenv environment when entering the directory

## Deploying to the "debian" Machine

To deploy to the "debian" machine (thinkcenter-1) via Tailscale SSH:

```bash
mage recreateNode thinkcenter-1
```

This will deploy the NixOS configuration to the existing machine via Tailscale SSH. Make sure the AGE_PRIVATE_KEY environment variable is set in your .env file for secrets decryption.

### Note on Hardware Configuration

The hardware-configuration.nix file for the machine will be created by nixos-anywhere during the installation process. You don't need to create this file manually.

If you're getting an error about missing hardware-configuration.nix when running `nix flake check`, this is expected before the first deployment. The file will be created during the deployment process.
</file>

<file path="rehtsetr">
Commit ID: [38;5;4m6f3fc6655b60bb15dd5ad090e2efce70b54549bc[39m
Change ID: [38;5;5mmwyumlrvuksmlmyunsmqwzyqstwpzzom[39m
Bookmarks: [38;5;5mpush-mwyumlrvuksm[39m [38;5;5mpush-mwyumlrvuksm@git push-mwyumlrvuksm@origin[39m
Author   : [38;5;3mEvan Hatch[39m <[38;5;3mevanlhatch@gmail.com[39m> ([38;5;6m2025-05-15 00:19:48[39m)
Committer: [38;5;3mEvan Hatch[39m <[38;5;3mevanlhatch@gmail.com[39m> ([38;5;6m2025-05-15 00:25:11[39m)

    fix boot.initrd.lvm

flake.lock                           |  4 [38;5;2m++[38;5;1m--[39m
flake.nix                            | 29 [38;5;2m++++++++++++[38;5;1m-----------------[39m
k3s-cluster/profiles/base-server.nix | 33 [38;5;2m++++++++++[38;5;1m-----------------------[39m
3 files changed, 24 insertions(+), 42 deletions(-)
</file>

<file path="sops.secrets.yaml">
ADMIN_SSH_PUBLIC_KEY: ENC[AES256_GCM,data:wR9unjg2z0q+MgiVssa3TtgTsDbFT+JazhLjTLnfmgL1lY22xIeO/Xl1nMUzelWHsNgQkBVRV7Rd+XELyYhskmX3tIbOvqRw6JJ9ZgKyt5cmmLsHzdeeeTL17VhY0x+IPi4L155Xo950kt5sfBiREC25NAqVwyeSziMtVl0XxTiZbn/5j5KeB5CuIytNO0ldcTZImi5Pfh3BcqwrpkZzJ6KYOYPsF+vc1CYd2JY0ONaDPdDdV4xXMSUBJCZdS0z2oj6/SpoMb5H3hJqcypqZK/y0i9jFSNfEtay1gXdbl2sYkmk7Vhh4zbc4tmjhUV6ZKHdJ3ISiiLY9nPnLisqmtHrUACXzMLB4bY2FFqFNKeMTykcN52zFArSZ/MljIxfkAXBmi2K/NAvhEx6ss7aUw3jLXoHEnEsod6WLx576Z86GPSxkYcbKqQEUIWIJ1BtA87VH3dEccSZDafQiV1Oh8izQYmNvGFHDVJIZTwfq5tJ5ZnotXtYjFQPxtxc=,iv:p+HdBL88qwWQ3j1U1/hLsliad5n/z+Rx7eNhqibcNTE=,tag:CvFFy68qrpZ9AdegrdEYZQ==,type:str]
TAILSCALE_AUTH_KEY: ENC[AES256_GCM,data:PS4DcjGBA0FqaGtkdT7HsGwKrBGrD5GQeZQJDUYy+bN9t5bmmZELh27VGwJkfv654A4MzCKLF1OJf6H/kA==,iv:3IflyCdYB7ncUW+G3ZbAUcDnFWtPZifyHeDAjZhrv84=,tag:ppZV/pLPJxFSmsBHJnKLVA==,type:str]
INFISICAL_CLIENT_ID: ENC[AES256_GCM,data:ADZjosL0Fv4UPz5m7r/DbVGgFSyhIHFhaq8OM9mLFvJ+OREf,iv:BHBePiRE2zI4HlHRhQseSyAhURjZ26Han5sfKo7swBQ=,tag:90FrSZhMAqAnO0kdBOoIsA==,type:str]
INFISICAL_CLIENT_SECRET: ENC[AES256_GCM,data:/WcMX5JzGnYPqKOr4YBWZL56okxbJro0BXZKIPkO1tHUEBdbfsqdf/DBnckPPD+WFHnNpWzCzueBVCbRLfntxA==,iv:D4EQSI+Hgvgsy27ZFfKEerNueNHxCNnc9tjLuGpf6Ts=,tag:GC8V+ZsEhZue84WBgHr9/w==,type:str]
INFISICAL_ADDRESS: ENC[AES256_GCM,data:f1sY6WJIMCoMSSOMpFWsZr3jPIiTKMyxew==,iv:hSXryQyrccbOLvthgTWRzMQ++A2pOWYbouOHLi2YWhM=,tag:keS2yImqG6rWBzM7C/bd8g==,type:str]
K3S_TOKEN: ENC[AES256_GCM,data:KG1/gNT9wKfoc+EJuyadVN8=,iv:hs0GsbgmgElJN6M3v1Y+uSFNwloaAKqlIUWwXEHG6Gg=,tag:n62MZt/OnEhJMfE3bfTfbQ==,type:str]
HETZNER_TOKEN: ENC[AES256_GCM,data:z9z3l2xX2eMcX5eIKEllOwWYRRwjqNkz53LMcYcyGVyfchzf0YmcR+uEoXcKthnKjykf82fpJ4BWj1Pg4Ns6KA==,iv:LwXB8LW/GxK8NyVCOUJ63HwEMDZBfAla/UmPn7jn3bc=,tag:LADXmf1AoSlylfdWPhqr1A==,type:str]
GITHUB_TOKEN: ENC[AES256_GCM,data:gfy1QFMBminrETR5X8lPvBqLvyx+AydPPwoNSD/w2SF2qdft5+1/Y91rjxThJbzBuRI8adb1lBNqk3UourBwC0zXA6kHx12XzVuIdg905egx2QjNLNRmq2jN/A18,iv:lX5pgKIKrZJ42efs5mHiealXcHdxuRUFL46uhpK7Fhk=,tag:yWaXVIEXFD7TwKwiv7qODg==,type:str]

sops:
  age:
    - recipient: age1p5vdpde60kwfjqeyufvp8xxtfk4ja39h42v4y26u8ju85ysw9y4qvuh0cd
      enc: |
        -----BEGIN AGE ENCRYPTED FILE-----
        YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSAwQTFnT2pvbmFoakxxVTJ2
        cjBRbGVVbXIydFRlcWVnNW1tbjltWVg1OGdVCjRRekdHQllVNXpaK0w0Yk9vZVA3
        NFB2SG91M2VnVVk1cFpNVzAwbEdDK2MKLS0tIGhjd3ovN2tRQTk0ZWNXYi9ocDJU
        cDBSaDQxT0VuN0V3MlRKM0VWeXRlWE0K2D86QORMHPAm71kiqxWP8FqG4Y1gqi4Q
        bAPzbF9DBRn+0oBAQ8IoSQjIzZweTTTvOeQBjmGKyiDUtkq1VP35Mg==
        -----END AGE ENCRYPTED FILE-----
  lastmodified: "2025-05-13T19:44:19Z"
  mac: ENC[AES256_GCM,data:kA1eswgbZDUe0fJg4HP5Nz29gjmS8oZsD8iu5OHXpDoEMTN9eV2EqLL9tcEh+fZZwpG1agG59fH1UoNsg1LBc0VCAetn1u/fzYkEw+nt9enGic9bSDXpHw/BIXGBQtQ1+y+j6VfyTW+mHXccSorQwZGg1FVpyULvlnJagitF6LQ=,iv:+9qdXbPJXG7t51f63QF/yt6NrsV0+uKcmZhey9YXuN4=,tag:2x0faN2TNQVsI7JISTvnzw==,type:str]
  unencrypted_suffix: _unencrypted
  version: 3.10.2
</file>

<file path="disko-configs/generic-disko-layout.nix">
# Modified disko layout for thinkcenter-1 with LVM and UEFI-only
{
  config,
  lib,
  pkgs,
  ...
}:

{
  disko.devices = {
    disk = {
      mainDisk = {
        type = "disk";
        device = "/dev/sda";
        content = {
          type = "gpt";
          partitions = {
            esp = {
              name = "ESP";
              size = "1G";
              type = "EF00"; # EFI System Partition
              content = {
                type = "filesystem";
                format = "vfat";
                mountpoint = "/boot";
              };
            };
            # LVM Physical Volume
            lvm_pv = {
              name = "LVM_PV_MAIN";
              size = "100%"; # Use all remaining space
              content = {
                type = "lvm_pv";
                vg = "vg_main"; # Assign this PV to vg_main
              };
            };
          };
        };
      };
    };
    lvm_vg = {
      vg_main = {
        # Define the Volume Group
        type = "lvm_vg";
        lvs = {
          # Logical Volume for Root Filesystem
          root = {
            name = "lv_root";
            size = "100%FREE"; # All free space in VG
            content = {
              type = "filesystem";
              format = "ext4";
              mountpoint = "/";
              mountOptions = [
                "defaults"
                "discard"
              ]; # Add discard for SSDs if appropriate
            };
          };
          # Logical Volume for Swap
          swap = {
            name = "lv_swap";
            size = "8G"; # 8GB Swap LV
            content = {
              type = "swap";
            };
          };
        };
      };
    };
  };
}
</file>

<file path="k3s-cluster/locations/local.nix">
{
  config,
  lib,
  pkgs,
  ...
}:

{
  # Local machine specific configuration

  # Use NetworkManager for networking on local machines
  networking = {
    useDHCP = false;
    networkmanager.enable = true;
  };

  # Enable DHCP on all interfaces by default
  # Override this in the hardware-configuration.nix if needed
  networking.interfaces = lib.mkDefault {
    # This is a placeholder that will be overridden by hardware-configuration.nix
  };

  # Local machine specific boot settings
  boot.loader = {
    systemd-boot = {
      enable = true;
      configurationLimit = 10;
    };
    efi.canTouchEfiVariables = true;
  };

  # Enable firmware updates
  hardware.enableRedistributableFirmware = true;

  # Disable non-redistributable firmware for now
  # hardware.enableAllFirmware = true;

  # Enable CPU microcode updates
  hardware.cpu.intel.updateMicrocode = lib.mkDefault config.hardware.enableRedistributableFirmware;
  hardware.cpu.amd.updateMicrocode = lib.mkDefault config.hardware.enableRedistributableFirmware;

  # Enable fstrim for SSDs
  services.fstrim.enable = true;

  # Enable smartd for disk monitoring
  services.smartd = {
    enable = true;
    autodetect = true;
    notifications.mail.enable = false;
  };

  # Enable thermald for thermal management
  services.thermald.enable = true;

  # Enable TLP for power management
  services.tlp.enable = true;

  # Enable powertop
  powerManagement.powertop.enable = true;

  # Enable auto-cpufreq
  services.auto-cpufreq.enable = true;

  # Enable firewall
  networking.firewall = {
    enable = true;
    allowedTCPPorts = [ 22 ]; # SSH
    allowPing = true;
  };

  # Enable avahi for local network discovery
  services.avahi = {
    enable = true;
    nssmdns4 = true;
    publish = {
      enable = true;
      addresses = true;
      domain = true;
      hinfo = true;
      userServices = true;
      workstation = true;
    };
  };

  # Enable mDNS
  services.resolved = {
    enable = true;
    dnssec = "false";
    domains = [ "~." ];
    fallbackDns = [
      "1.1.1.1"
      "8.8.8.8"
    ];
    extraConfig = ''
      MulticastDNS=yes
    '';
  };

  # Enable NTP
  services.timesyncd.enable = true;

  # Set timezone to local timezone (override in hardware-configuration.nix if needed)
  time.timeZone = lib.mkDefault "America/Denver";
}
</file>

<file path="k3s-cluster/common.nix">
# ./k3s-cluster/common.nix
# Contains settings common to ALL NixOS systems built from this flake.
{
  config,
  lib,
  pkgs,
  ...
}: # specialArgs are available via config.specialArgs

{

  # \----- Hostname Configuration -----

  # Set from specialArgs.hostname, which defaults to the machine's name from machines.nix (via flake.nix)
  networking.hostName = lib.mkDefault config.specialArgs.hostname;

  # \----- Localization -----

  time.timeZone = "Etc/UTC"; # Consider setting your actual timezone, e.g., "America/Denver"
  i18n.defaultLocale = "en\_US.UTF-8";
  console.keyMap = "us";

  # \----- Admin User Account -----

  # Create the admin user specified in commonNodeArguments (via specialArgs)
  users.users.${config.specialArgs.adminUsername} = {
    isNormalUser = true;
    extraGroups = [
      "wheel"
      "networkmanager"
      "docker"
    ];
    openssh.authorizedKeys.keys = [ config.specialArgs.adminSshPublicKey ];
    shell = pkgs.fish;
  };

  # Add the same admin SSH key to the root user for initial provisioning or recovery

  users.users.root.openssh.authorizedKeys.keys = [ config.specialArgs.adminSshPublicKey ];

  # Sudo privileges for the wheel group (adminUser is typically in 'wheel')

  security.sudo.wheelNeedsPassword = false; # Set to true to require password for sudo

  # \----- Global Nix Configuration -----

  nixpkgs.config.allowUnfree = true; # Allow unfree packages if needed globally

  nix = {
    package = pkgs.nixFlakes; # Ensures the Flakes-aware Nix package is used
    settings = {
      auto-optimise-store = true;
      experimental-features = [
        "nix-command"
        "flakes"
      ];
      # Add the admin user to trusted-users to perform Nix operations without sudo (e.g., nix build)
      trusted-users = [
        "root"
        "@wheel"
        config.specialArgs.adminUsername
      ];
    };
    # Automatic garbage collection
    gc = {
      automatic = true;
      dates = "weekly";
      options = "--delete-older-than 30d";
    };
  };

  # \----- Time Synchronization -----

  services.timesyncd.enable = true; # Essential for most systems

  # \----- Basic System Environment -----

  # Minimal set of universally useful tools; more can go in base-server.nix

  environment.systemPackages = with pkgs; [
    gitMinimal
    micro
    curl
    wget
  ];

  environment.variables.EDITOR = "micro";
}
</file>

<file path=".envrc">
# .envrc
echo "Loading environment from .env..."
set -a # Automatically export all variables defined from now on
if [ -f .env ]; then
  source .env
else
  echo "Warning: .env file not found. Some configurations might be missing."
fi
set +a # Stop automatically exporting

# You can add 'use flake .' here if you want direnv to manage your dev shell
# if has nix_direnv_support && ! in_nix_shell; then
#  use flake .
# fi

echo "NixOS K3s Cluster environment ready."
</file>

<file path=".gitignore">
# Environment variables file - CONTAINS SECRETS, MUST BE IGNORED
.env

# Devenv files - Local development environment state
.devenv*
devenv.local.nix

# Direnv files - Shell environment loader state
.direnv

# Nix build outputs - Symlinks created by nix build
result
result-*

# Go temporary directories - Created by Go tools
# .go/ # Uncomment if your Go workflow creates a .go directory in the root

# Editor/IDE files - Local IDE settings or temporary files
.vscode/
.idea/
*.swp
*.swo
*.bak
*~

# OS generated files - macOS and Windows temp files
.DS_Store
Thumbs.db

# nixos-anywhere command history - May contain paths to temporary secret files
# Consider ignoring if you run this command frequently and don't want history tracked
nixos-anywhere commands.md
*.lock
machines.nix
.env
.envrc.local
</file>

<file path="devenv.nix">
{ pkgs, ... }:

{
  # Enable devenv shell features
  packages = with pkgs; [
    # Core tools
    go
    mage

    # Deployment tools
    deploy-rs
    nixos-anywhere
    disko

    # Cloud tools
    hcloud
    kubectl
    kubernetes-helm
    fluxcd
    tailscale
  ];

  # Set up environment variables
  env = {
    GOPATH = "$HOME/go";
    PATH = "$GOPATH/bin:$PATH";
    GO111MODULE = "on";
  };

  # Load environment variables from .env file
  dotenv.enable = true;
  dotenv.filename = ".env";

  # Pre-install Go dependencies
  enterShell = ''
    echo "K3s NixOS Configs Development Environment"
    echo "Installing Go dependencies..."
    go install github.com/joho/godotenv@latest
    go install github.com/magefile/mage@latest
    echo "Available mage commands:"
    mage -l 2>/dev/null || echo "mage not installed or no targets defined."
  '';

  # Scripts that can be run with `devenv up <name>`
  scripts = {
    deploy.exec = "mage deploy $@";
    recreate-node.exec = "mage recreateNode $@";
    recreate-server.exec = "mage recreateServer $@";
    delete-and-redeploy-server.exec = "mage deleteAndRedeployServer $@";
  };

  # Enter the development environment with the project directory as the working directory
  enterShell = ''
    echo "K3s NixOS Configs Development Environment"
    echo "Available mage commands:"
    mage -l 2>/dev/null || echo "mage not installed or no targets defined."
  '';

  # Ensure the project directory is the working directory
  processes = {
    # You can define long-running processes here if needed
  };
}
</file>

<file path="magefile.go">
//go:build mage
// +build mage

package main

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/joho/godotenv"    // For loading .env files
	"github.com/magefile/mage/mg" // mg contains helper functions for Mage
	"github.com/magefile/mage/sh" // sh allows running shell commands
)

func init() {
	// Load .env file if it exists
	err := godotenv.Load()
	if err != nil {
		fmt.Println("INFO: .env file not found or failed to load, using existing environment variables")
	} else {
		fmt.Println("INFO: .env file loaded successfully")

		// Export critical environment variables if they're not already set
		criticalEnvVars := []string{
			"AGE_PRIVATE_KEY",
			"K3S_TOKEN",
			"TAILSCALE_AUTH_KEY",
			"HCLOUD_TOKEN",
			"GITHUB_TOKEN",
		}

		// Read all variables from .env file
		envMap, err := godotenv.Read()
		if err != nil {
			fmt.Println("ERROR: Failed to read .env file:", err)
			return
		}

		// Export critical variables if they're in the .env file and not already set
		for _, envVar := range criticalEnvVars {
			if value, exists := envMap[envVar]; exists && os.Getenv(envVar) == "" {
				os.Setenv(envVar, value)
				fmt.Printf("INFO: Exported %s from .env file\n", envVar)
			}
		}
	}
}

// Default target executed when `mage` is run without arguments.
var Default = CheckFlake

// NodeMap defines the mapping between flake configuration names and their target hosts.
// You should populate this map with your actual nodes.
var nodeMap = map[string]string{
	"cpx21-control-1": "root@5.161.241.28",
	"thinkcenter-1":   "root@100.108.23.65", // Use root user for Tailscale access
	// Add other nodes here, e.g.:
	// "my-hcloud-control01": "root@your_other_node_ip",
}

// CheckFlake runs `nix flake check` to validate the flake.
func CheckFlake() error {
	fmt.Println("INFO: Checking Nix flake...")
	return sh.RunV("nix", "flake", "check", "--show-trace")
}

// UpdateFlake runs `nix flake update` to update all flake inputs.
func UpdateFlake() error {
	fmt.Println("INFO: Updating flake inputs...")
	return sh.RunV("nix", "flake", "update")
}

// ShowFlake runs `nix flake show`.
func ShowFlake() error {
	fmt.Println("INFO: Showing flake outputs...")
	return sh.RunV("nix", "flake", "show")
}

// Deploy deploys a given NixOS configuration to its target host using deploy-rs.
// Usage: mage deploy <flakeConfigName>
// Example: mage deploy cpx21-control-1
func Deploy(flakeConfigName string) error {
	mg.SerialDeps(CheckFlake) // Ensure flake is valid before deploying

	targetHost, ok := nodeMap[flakeConfigName]
	if !ok || targetHost == "" {
		return fmt.Errorf("ERROR: Flake configuration name '%s' not found in nodeMap or target host is empty. Please define it in magefile.go", flakeConfigName)
	}

	fmt.Printf("INFO: Deploying NixOS configuration '%s' to %s via deploy-rs...\n", flakeConfigName, targetHost)
	// Note: deploy-rs uses the hostname defined in the `deploy.nodes.<node>.hostname` attribute in flake.nix for SSH connection.
	// The targetHost from nodeMap is mostly for informational purposes here or if you were to construct ssh commands directly.
	return sh.RunV("deploy-rs", ".#"+flakeConfigName)
}

// Rebuild performs a nixos-rebuild switch on a target node.
// Usage: mage rebuild <flakeConfigName>
// Example: mage rebuild cpx21-control-1
func Rebuild(flakeConfigName string) error {
	targetHost, ok := nodeMap[flakeConfigName]
	if !ok || targetHost == "" {
		return fmt.Errorf("ERROR: Flake configuration name '%s' not found in nodeMap or target host is empty. Please define it in magefile.go", flakeConfigName)
	}

	fmt.Printf("INFO: Rebuilding NixOS configuration '%s' on %s...\n", flakeConfigName, targetHost)
	fmt.Println("IMPORTANT: This assumes your flake source (e.g., from git) is up-to-date on the target machine if it pulls from there.")
	// You might need to adjust the path to your flake on the remote machine.
	// Example assumes it's cloned in /root/k3s-nixos-configs
	cmd := fmt.Sprintf("ssh %s \"cd /root/k3s-nixos-configs && git pull && nixos-rebuild switch --flake .#%s\"", targetHost, flakeConfigName)
	return sh.RunV("bash", "-c", cmd)
}

// RecreateNode redeploys a node using nixos-anywhere and fetches its K3s config.
// This is a more destructive operation and re-images the server.
// Usage: mage recreateNode <flakeConfigName>
// Example: mage recreateNode cpx21-control-1
func RecreateNode(flakeConfigName string) error {
	targetHostVal, ok := nodeMap[flakeConfigName]
	if !ok || targetHostVal == "" {
		return fmt.Errorf("ERROR: Flake configuration name '%s' not found in nodeMap or target host is empty. Please define it in magefile.go", flakeConfigName)
	}

	// Extract user and host for nixos-anywhere, assuming format user@host
	parts := strings.SplitN(targetHostVal, "@", 2)
	if len(parts) != 2 {
		return fmt.Errorf("ERROR: targetHost '%s' for '%s' is not in user@host format", targetHostVal, flakeConfigName)
	}
	targetUser := parts[0]
	targetIP := parts[1]

	// Get SSH key path if provided
	sshKey := os.Getenv("MAGE_SSH_KEY") // e.g., "~/.ssh/id_rsa"

	// Only process the SSH key if it's provided
	if sshKey != "" {
		// Expand ~ to home directory if present
		if strings.HasPrefix(sshKey, "~/") {
			home, err := os.UserHomeDir()
			if err != nil {
				return fmt.Errorf("failed to get home directory: %w", err)
			}
			sshKey = filepath.Join(home, sshKey[2:])
		}

		// Verify SSH key exists
		if _, err := os.Stat(sshKey); os.IsNotExist(err) {
			fmt.Printf("WARNING: SSH key %s does not exist, proceeding without it\n", sshKey)
			sshKey = "" // Clear the SSH key if it doesn't exist
		} else {
			fmt.Printf("INFO: Using SSH key: %s\n", sshKey)
		}
	} else {
		fmt.Println("INFO: No SSH key provided, proceeding without it")
	}

	kubeconfigPath := fmt.Sprintf("%s/.kube/config-%s", os.Getenv("HOME"), flakeConfigName)

	fmt.Printf("INFO: Recreating and deploying '%s' to %s (IP: %s) using nixos-anywhere...\n", flakeConfigName, targetUser, targetIP)

	// Create a temporary directory to store the AGE key
	tempDir, err := os.MkdirTemp("", "nixos-anywhere-age-key")
	if err != nil {
		return fmt.Errorf("failed to create temporary directory for AGE key: %w", err)
	}
	defer os.RemoveAll(tempDir) // Clean up when done

	// Create the directory structure for the AGE key
	ageKeyDir := filepath.Join(tempDir, "etc", "sops", "age")
	if err := os.MkdirAll(ageKeyDir, 0700); err != nil {
		return fmt.Errorf("failed to create AGE key directory: %w", err)
	}

	// Get the AGE key from environment
	ageKey := os.Getenv("AGE_PRIVATE_KEY")
	if ageKey == "" {
		return fmt.Errorf("AGE_PRIVATE_KEY environment variable is not set")
	}

	// Ensure the AGE key has the correct format (should start with AGE-SECRET-KEY-)
	if !strings.HasPrefix(ageKey, "AGE-SECRET-KEY-") {
		return fmt.Errorf("AGE_PRIVATE_KEY has invalid format, should start with AGE-SECRET-KEY-")
	}

	// Write the AGE key to a file
	ageKeyPath := filepath.Join(ageKeyDir, "key.txt")
	if err := os.WriteFile(ageKeyPath, []byte(ageKey), 0600); err != nil {
		return fmt.Errorf("failed to write AGE key: %w", err)
	}

	fmt.Printf("INFO: AGE key written to %s\n", ageKeyPath)

	// Run nixos-anywhere to deploy NixOS to the target machine
	// Hardware configuration will be generated at runtime using facter
	fmt.Printf("INFO: Running nixos-anywhere to deploy NixOS to %s@%s\n", targetUser, targetIP)

	// Build command arguments
	nixosAnywhereArgs := []string{
		"nixos-anywhere",
		"--debug",                    // Enable debug output
		"-f", ".#" + flakeConfigName, // Use -f instead of --flake
	}

	// Add other options
	nixosAnywhereArgs = append(nixosAnywhereArgs,
		"--extra-files", tempDir, // Copy the AGE key to the target machine
		"--substitute-on-destination", // Enable substitutes on the destination
		"--copy-host-keys",            // Copy existing SSH host keys to maintain SSH identity
	)

	// Always use the SSH key if provided
	if sshKey != "" {
		nixosAnywhereArgs = append(nixosAnywhereArgs, "-i", sshKey)
	}

	// Add the target host as the last argument
	nixosAnywhereArgs = append(nixosAnywhereArgs, targetUser+"@"+targetIP)

	fmt.Printf("INFO: Running nixos-anywhere with args: %v\n", nixosAnywhereArgs)

	// Set environment variables for the command
	env := map[string]string{
		"AGE_PRIVATE_KEY": os.Getenv("AGE_PRIVATE_KEY"),
	}

	// Run the command
	err = sh.RunWithV(env, nixosAnywhereArgs[0], nixosAnywhereArgs[1:]...)
	if err != nil {
		return fmt.Errorf("nixos-anywhere deployment failed: %w", err)
	}

	fmt.Printf("INFO: Waiting for %s to reboot and become available...\n", targetIP)
	// Add a sleep or a more sophisticated check for server availability here
	sh.Run("sleep", "30") // Basic wait, adjust as needed

	fmt.Println("INFO: Copying K3s configuration file from the server...")

	// Build SSH command arguments
	sshArgs := []string{
		"-o", "StrictHostKeyChecking=no",
		"-o", "UserKnownHostsFile=/dev/null",
	}

	// Add SSH key if provided
	if sshKey != "" {
		sshArgs = append(sshArgs, "-i", sshKey)
	}

	// Add target host and command
	sshArgs = append(sshArgs, targetHostVal, "sudo cat /etc/rancher/k3s/k3s.yaml")

	// Execute SSH command
	k3sConfigContent, err := sh.Output("ssh", sshArgs...)
	if err != nil {
		return fmt.Errorf("failed to copy k3s.yaml: %w", err)
	}

	// Create directory if it doesn't exist
	if err := os.MkdirAll(filepath.Dir(kubeconfigPath), 0755); err != nil {
		return fmt.Errorf("failed to create kubeconfig directory: %w", err)
	}

	if err := os.WriteFile(kubeconfigPath, []byte(k3sConfigContent), 0600); err != nil {
		return fmt.Errorf("failed to write kubeconfig: %w", err)
	}

	fmt.Printf("INFO: K3s config copied to %s\n", kubeconfigPath)
	fmt.Printf("INFO: To use it, run: export KUBECONFIG=%s\n", kubeconfigPath)
	fmt.Printf("INFO: Node '%s' recreated and configured. Tailscale and K3s should be setting up.\n", flakeConfigName)
	return nil
}

// RecreateServer recreates a Hetzner Cloud server with the specified properties.
// Usage: mage recreateServer <serverName> <ipv4Enabled (true/false)>
// Example: mage recreateServer cpx21-control-1 true
func RecreateServer(serverName string, ipv4Enabled string) error {
	mg.SerialDeps(CheckFlake) // Ensure flake is valid before recreating the server

	// Get required environment variables
	hcloudToken := os.Getenv("HCLOUD_TOKEN")
	if hcloudToken == "" {
		return fmt.Errorf("ERROR: HCLOUD_TOKEN environment variable must be set")
	}

	sshKeyName := os.Getenv("HETZNER_SSH_KEY_NAME")
	if sshKeyName == "" {
		return fmt.Errorf("ERROR: HETZNER_SSH_KEY_NAME environment variable must be set")
	}

	privateNetName := os.Getenv("PRIVATE_NETWORK_NAME")
	if privateNetName == "" {
		return fmt.Errorf("ERROR: PRIVATE_NETWORK_NAME environment variable must be set")
	}

	placementGroupName := os.Getenv("PLACEMENT_GROUP_NAME")
	if placementGroupName == "" {
		return fmt.Errorf("ERROR: PLACEMENT_GROUP_NAME environment variable must be set")
	}

	// Get optional environment variables with defaults
	hetznerLocation := os.Getenv("HETZNER_LOCATION")
	if hetznerLocation == "" {
		hetznerLocation = "ash"
		fmt.Printf("INFO: HETZNER_LOCATION not set, defaulting to %s\n", hetznerLocation)
	}

	imageName := os.Getenv("HETZNER_IMAGE_NAME")
	if imageName == "" {
		imageName = "debian-12"
		fmt.Printf("INFO: HETZNER_IMAGE_NAME not set, defaulting to %s\n", imageName)
	}

	serverType := os.Getenv("CONTROL_PLANE_VM_TYPE")
	if serverType == "" {
		serverType = "cpx21"
		fmt.Printf("INFO: CONTROL_PLANE_VM_TYPE not set, defaulting to %s\n", serverType)
	}

	// Construct datacenter name from location
	datacenterName := fmt.Sprintf("%s-dc1", hetznerLocation)

	// Convert ipv4Enabled string to boolean
	var enableIPv4 bool
	if ipv4Enabled == "" {
		// Check environment variable if parameter not provided
		enableIPv4Env := os.Getenv("HETZNER_DEFAULT_ENABLE_IPV4")
		if strings.ToLower(enableIPv4Env) == "true" {
			enableIPv4 = true
		} else {
			enableIPv4 = false
		}
	} else if strings.ToLower(ipv4Enabled) == "true" {
		enableIPv4 = true
	} else if strings.ToLower(ipv4Enabled) == "false" {
		enableIPv4 = false
	} else {
		return fmt.Errorf("ERROR: ipv4Enabled must be either 'true' or 'false'")
	}

	fmt.Printf("INFO: Recreating server %s with IPv4 enabled: %t...\n", serverName, enableIPv4)

	// 1. Delete the existing server
	fmt.Println("INFO: Deleting existing server...")
	err := sh.RunV("hcloud", "server", "delete", serverName, "--force")
	if err != nil && !strings.Contains(err.Error(), "Not Found") {
		return fmt.Errorf("failed to delete server: %w", err)
	}

	// 2. Create a new server with the same properties
	fmt.Println("INFO: Creating new server...")

	var createArgs []string
	createArgs = append(createArgs, "server", "create", serverName)
	createArgs = append(createArgs, "--server-type", serverType)
	createArgs = append(createArgs, "--image", imageName)
	createArgs = append(createArgs, "--datacenter", datacenterName)
	createArgs = append(createArgs, "--ssh-key", sshKeyName)
	createArgs = append(createArgs, "--network", privateNetName)
	createArgs = append(createArgs, "--placement-group", placementGroupName)

	if enableIPv4 {
		createArgs = append(createArgs, "--enable-ipv4")
	}

	// The HCLOUD_TOKEN environment variable will be used automatically

	err = sh.RunV("hcloud", createArgs...)

	if err != nil {
		return fmt.Errorf("failed to create server: %w", err)
	}

	fmt.Printf("INFO: Server %s recreated successfully.\n", serverName)
	return nil
}

// DeleteAndRedeployServer deletes an existing server, recreates it, and then deploys NixOS to it.
// This combines RecreateServer and RecreateNode into a single operation.
// Usage: mage deleteAndRedeployServer <serverName> <flakeConfigName> <ipv4Enabled (optional)>
// Example: mage deleteAndRedeployServer cpx21-control-1 cpx21-control-1 true
func DeleteAndRedeployServer(serverName string, flakeConfigName string, ipv4Enabled string) error {
	fmt.Printf("INFO: Starting complete redeployment of server %s with flake config %s\n", serverName, flakeConfigName)

	// Step 1: Recreate the server
	if err := RecreateServer(serverName, ipv4Enabled); err != nil {
		return fmt.Errorf("failed to recreate server: %w", err)
	}

	// Wait for the server to be fully up
	fmt.Println("INFO: Waiting for server to be fully up...")
	sh.Run("sleep", "30") // Basic wait, adjust as needed

	// Step 2: Deploy NixOS to the server
	if err := RecreateNode(flakeConfigName); err != nil {
		return fmt.Errorf("failed to deploy NixOS to server: %w", err)
	}

	fmt.Printf("INFO: Server %s has been successfully deleted, recreated, and redeployed with NixOS configuration %s\n",
		serverName, flakeConfigName)
	return nil
}

// DecryptSecrets decrypts the sops.secrets.yaml file and prints its content.
// Requires the AGE_PRIVATE_KEY environment variable to be set.
func DecryptSecrets() error {
	fmt.Println("INFO: Decrypting sops.secrets.yaml...")

	// The init() function should have loaded AGE_PRIVATE_KEY from .env
	ageKey := os.Getenv("AGE_PRIVATE_KEY")
	if ageKey == "" {
		// Check again, in case init() didn't run or .env was missing the key
		fmt.Println("ERROR: AGE_PRIVATE_KEY environment variable is not set.")
		fmt.Println("Please ensure it is defined in your .env file and you have run 'direnv allow' or manually exported it.")
		return fmt.Errorf("AGE_PRIVATE_KEY not set")
	}

	// sops expects the key in SOPS_AGE_KEY, so we set it for the sops command.
	env := map[string]string{
		"SOPS_AGE_KEY": ageKey,
	}

	// Run the sops decrypt command
	err := sh.RunWithV(env, "sops", "--decrypt", "sops.secrets.yaml")
	if err != nil {
		return fmt.Errorf("failed to decrypt sops.secrets.yaml: %w", err)
	}

	fmt.Println("INFO: Decryption complete.")
	fmt.Println("WARNING: The decrypted secrets were printed to your console. Be mindful of your environment.")

	return nil
}

// Alias for CheckFlake
var Check = CheckFlake

// Helper function to get a filepath, used by RecreateNode
// Not directly used by user, but good to have if needed for other funcs
func getDir(path string) string {
	return filepath.Dir(path)
}

// DeployControlNode deploys a self-hosted control node to the Debian machine at 100.108.23.65
// This is a convenience function that calls RecreateNode with the thinkcenter-1 configuration
// Usage: mage deployControlNode
func DeployControlNode() error {
	fmt.Println("INFO: Deploying self-hosted control node to Debian machine at 100.108.23.65...")

	// Check if AGE_PRIVATE_KEY is set
	if os.Getenv("AGE_PRIVATE_KEY") == "" {
		fmt.Println("WARNING: AGE_PRIVATE_KEY environment variable is not set. Make sure it's in your .env file.")
	}

	// Call RecreateNode with the thinkcenter-1 configuration
	return RecreateNode("thinkcenter-1")
}
</file>

<file path="k3s-cluster/profiles/base-server.nix">
# ./k3s-cluster/profiles/base-server.nix
{
  config,
  lib,
  pkgs,
  inputs ? { }, # Available if passed by flake's mkNixosSystem specialArgs
  ...
}:

{
  imports = [
    ../common.nix # Imports global settings like hostname, admin user, base Nix config
                  # Adjust path if common.nix is located elsewhere relative to this file.
    # Modules specific to your k3s cluster servers:
    ../modules/tailscale.nix
    ../modules/infisical-agent.nix # Conditionally enabled via specialArgs from flake.nix
    ../modules/netdata.nix
    # Disko layout and hardware-configuration.nix are handled by flake.nix's mkNixosSystem
  ];

  # ----- LVM Support for Boot -----
  # Crucial if your Disko configuration uses LVM for the root filesystem
  boot.initrd.lvm.enable = true; # << CORRECTED LINE

  # ----- Server-Specific System Configuration -----
  boot.tmp.cleanOnBoot = true;
  zramSwap.enable = true;
  # Firewall is enabled here; specific rules can be added by roles/locations or directly here.
  networking.firewall.enable = true;

  # Server-Specific Packages (common.nix provides a more minimal set)
  environment.systemPackages = with pkgs; [
    # System tools
    lsof
    htop
    iotop
    sysstat # for iostat, mpstat etc.
    tcpdump
    iptables # For viewing/diagnosing firewall rules, even if nftables is the backend

    # File tools
    file
    tree
    ncdu
    ripgrep
    fd

    # Network tools
    inetutils # Provides ping, hostname, etc.
    mtr
    nmap
    socat

    # Process management
    psmisc # Provides pstree, killall, etc.
    procps # Provides ps, top, free, etc.

    # Text processing
    jq
    yq-go # Using go version of yq as it's often preferred
  ];

  # ----- Server-Specific Service Configuration -----
  services.openssh = {
    enable = true; # Ensure SSHD is running on servers
    settings = {
      X11Forwarding = false;
      AllowTcpForwarding = true; # Useful for kubectl port-forward, etc. Review security implications.
      PermitRootLogin = "prohibit-password"; # Root login with key only (key setup in common.nix)
      PasswordAuthentication = false;        # Disable password-based SSH login entirely
      KbdInteractiveAuthentication = false;  # Disable keyboard-interactive auth (often implies passwords)
      MaxAuthTries = 3;
    };
  };

  # Security hardening for servers
  security.auditd.enable = true; # Enable audit daemon
  security.audit.enable = true;  # Enable kernel audit system (used by auditd)

  # Networking (useDHCP=false for servers, useNetworkd for explicit config)
  networking.useDHCP = lib.mkDefault false; # Servers typically have static or well-defined IP configurations
  networking.useNetworkd = true;    # Use systemd-networkd for network configuration
  systemd.network.enable = true;    # Ensure the service itself is enabled

  networking.firewall = { # Specific firewall rules can be added by roles/locations or further down here
    allowPing = true;
    logReversePathDrops = true;
  };

  # Disable services typically not needed on a headless server
  services.xserver.enable = false;
  services.printing.enable = false;
  hardware.bluetooth.enable = false;
  sound.enable = false;
}
</file>

<file path="flake.nix">
# ./flake.nix
{
  description = "NixOS K3s Cluster Configuration";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    deploy-rs = {
      url = "github:serokell/deploy-rs";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    disko = {
      url = "github:nix-community/disko";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    nixos-anywhere = {
      url = "github:numtide/nixos-anywhere";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    sops-nix = {
      url = "github:Mic92/sops-nix";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    nixos-facter-modules = {
      url = "github:numtide/nixos-facter-modules"; # Corrected URL
      # Removed: inputs.nixpkgs.follows = "nixpkgs"; # Fixes warning
    };
  };

  outputs =
    {
      self,
      nixpkgs,
      deploy-rs,
      disko,
      sops-nix,
      nixos-anywhere,
      nixos-facter-modules,
      ...
    }@inputs:
    let
      system = "x86_64-linux";
      pkgs = nixpkgs.legacyPackages.${system};
      lib = nixpkgs.lib;

      getEnv =
        name: defaultValue:
        let
          value = builtins.getEnv name;
        in
        if value == "" then defaultValue else value;
      stateVersionModule =
        version:
        { ... }:
        {
          system.stateVersion = lib.mkDefault version;
        };

      commonSopsModule =
        {
          config,
          pkgs,
          lib,
          ...
        }:
        {
          sops.age.keyFile = "/etc/sops/age/key.txt";
          sops.age.generateKey = false;
          sops.validateSopsFiles = false;
          sops.secrets.infisical_client_id = { };
          sops.secrets.infisical_client_secret = { };
          sops.secrets.infisical_address = { };
          sops.secrets.K3S_CLUSTER_JOIN_TOKEN = { };
          sops.secrets.TAILSCALE_PROVISION_KEY = { };
          sops.defaultSopsFile = "/etc/nixos/secrets.sops.yaml";
          system.activationScripts.deploySopsFile =
            lib.mkIf (config.sops.defaultSopsFile != null && builtins.pathExists ./sops.secrets.yaml)
              {
                text = ''
                  echo "Copying encrypted sops file to target system..."
                  mkdir -p "$(dirname "${config.sops.defaultSopsFile}")"
                  cp ${./sops.secrets.yaml} "${config.sops.defaultSopsFile}"
                  chmod 0400 "${config.sops.defaultSopsFile}"
                  echo "Encrypted sops file deployed to ${config.sops.defaultSopsFile}"
                '';
                deps = [ "users" ];
              };
          systemd.tmpfiles.rules = [ "d /etc/sops/age 0700 root root -" ];
        };

      commonNodeArguments = {
        k3sControlPlaneAddr = getEnv "K3S_CONTROL_PLANE_ADDR" "https_REPLACE_ME_K3S_API_ENDPOINT_6443";
        adminUsername = getEnv "ADMIN_USERNAME" "nixos_admin";
        adminSshPublicKey = getEnv "ADMIN_SSH_PUBLIC_KEY" "ssh-ed25519 REPLACE_ME_WITH_YOUR_PUBLIC_KEY";
        nixosStateVersion = getEnv "NIXOS_STATE_VERSION" "25.05";
        hetznerPublicInterface = getEnv "HETZNER_PUBLIC_INTERFACE" "eth0";
        hetznerPrivateInterface = getEnv "HETZNER_PRIVATE_INTERFACE" "ens10";
      };

      rolePathMappings = {
        control = ./k3s-cluster/roles/k3s-control.nix;
        worker = ./k3s-cluster/roles/k3s-worker.nix;
      };
      locationProfilePathMappings = {
        hetzner = ./k3s-cluster/locations/hetzner.nix;
        local = ./k3s-cluster/locations/local.nix;
      };
      diskoConfigPathMappings = {
        hetzner = ./disko-configs/hetzner-disko-layout.nix;
        local = ./disko-configs/generic-disko-layout.nix;
      };

      dummyHardwareConfigPath = ./dummy-hardware-config.nix; # Ensure this file exists

      mkNixosSystem =
        {
          derivedRolePath,
          derivedLocationProfilePath,
          derivedDiskoConfigPath,
          hardwareConfigModulePath,
          extraModules ? [ ],
          specialArgsResolved,
        }:
        nixpkgs.lib.nixosSystem {
          inherit system;
          modules = [
            inputs.sops-nix.nixosModules.sops
            commonSopsModule
            ./k3s-cluster/profiles/base-server.nix
            derivedRolePath
            derivedLocationProfilePath
            inputs.disko.nixosModules.disko
            derivedDiskoConfigPath
            (stateVersionModule specialArgsResolved.nixosStateVersion)
            hardwareConfigModulePath
          ] ++ extraModules;
          specialArgs = specialArgsResolved;
        };

      privateMachinesPath = ./machines.nix;
      allMachinesData =
        if builtins.pathExists privateMachinesPath then
          (import privateMachinesPath {
            inherit
              lib
              pkgs
              getEnv
              stateVersionModule
              ;
          })
        else
          {
            "example-node" = {
              location = "local";
              nodeType = "control-init";
              _hardwareConfigModulePath_override = dummyHardwareConfigPath;
              deploy = {
                sshHostname = "localhost";
                sshUser = getEnv "USER" "nixos";
              };
            };
          };

    in
    {
      nixosConfigurations = lib.mapAttrs (
        name: machineData:
        let
          roleKey = if machineData.nodeType == "worker" then "worker" else "control";
          # Corrected line: removed the stray 'example' and ensured 'or (throw ...)' is applied correctly
          finalRolePath =
            rolePathMappings.${roleKey}
              or (throw "Invalid role derived from nodeType: '${machineData.nodeType}' for machine '${name}'. Must be 'control-init/join' or 'worker'.");

          finalLocationProfilePath =
            locationProfilePathMappings.${machineData.location}
              or (throw "Invalid location: '${machineData.location}' for machine '${name}'. Must be 'hetzner' or 'local'.");

          finalDiskoConfigPath =
            diskoConfigPathMappings.${machineData.location}
              or (throw "No disko config mapping for location: '${machineData.location}' for machine '${name}'.");

          isFirstCp = (machineData.nodeType == "control-init");

          resolvedSpecialArgs =
            commonNodeArguments
            // {
              location = machineData.location;
              isFirstControlPlane = isFirstCp;
            }
            // {
              hostname = name;
            }
            // (machineData.specialArgsOverride or { });

          finalHardwareConfigModulePath =
            machineData._hardwareConfigModulePath_override or /etc/nixos/hardware-configuration.nix;

        in
        mkNixosSystem {
          derivedRolePath = finalRolePath;
          derivedLocationProfilePath = finalLocationProfilePath;
          derivedDiskoConfigPath = finalDiskoConfigPath;
          hardwareConfigModulePath = finalHardwareConfigModulePath;
          extraModules = machineData.extraModules or [ ];
          specialArgsResolved = resolvedSpecialArgs;
        }
      ) allMachinesData;

      deploy.nodes = lib.mapAttrs (
        name: machineData:
        if !(machineData ? deploy) then
          null
        else
          {
            inherit (machineData.deploy) sshHostname sshUser;
            fastConnection = machineData.deploy.fastConnection or true;
            profiles.system = {
              user = machineData.deploy.activationUser or "root";
              path = deploy-rs.lib.${system}.activate.nixos self.nixosConfigurations."${name}";
            };
          }
      ) (lib.filterAttrs (name: data: data != null && data ? deploy) allMachinesData);

      packages.${system} =
        let
          mageEnvPath = lib.makeBinPath [
            pkgs.mage
            pkgs.go
            pkgs.hcloud
            pkgs.kubectl
            pkgs.kubernetes-helm
            pkgs.fluxcd
            pkgs.tailscale
            deploy-rs.packages.${system}.deploy-rs
            nixos-anywhere.packages.${system}.default
            inputs.nixos-facter-modules.packages.${system}.default # facter CLI
            disko.packages.${system}.disko
          ];
          buildDiskImage =
            imageName: nixosConfigName: format: diskSize:
            pkgs.callPackage (nixpkgs + "/nixos/lib/make-disk-image.nix") {
              name = imageName;
              inherit format diskSize;
              config = self.nixosConfigurations."${nixosConfigName}".config;
              inherit pkgs;
            };
          mageWrappers = pkgs.runCommand "mage-wrappers" { buildInputs = [ pkgs.makeWrapper ]; } ''
            mkdir -p $out/bin
            makeWrapper ${pkgs.mage}/bin/mage $out/bin/mage --set PATH "${mageEnvPath}" --run "cd ${toString ./.}";
            for target in recreateNode deploy recreateServer deleteAndRedeployServer; do
              makeWrapper ${pkgs.mage}/bin/mage $out/bin/mage-$target --set PATH "${mageEnvPath}" --run "cd ${toString ./.}"; --add-flags "$target";
            done
          '';
        in
        {
          hetznerK3sWorkerRawImage = buildDiskImage "hetzner-k3s-worker-image" "example-node" "raw" "20G";
          hetznerK3sControlRawImage = buildDiskImage "hetzner-k3s-control-image" "example-node" "raw" "20G";
          inherit mageWrappers;
        };

      apps.${system} = {
        mage = {
          type = "app";
          program = "${self.packages.${system}.mageWrappers}/bin/mage";
        };
        recreateNode = {
          type = "app";
          program = "${self.packages.${system}.mageWrappers}/bin/mage-recreateNode";
        };
        deploy = {
          type = "app";
          program = "${self.packages.${system}.mageWrappers}/bin/mage-deploy";
        };
        recreateServer = {
          type = "app";
          program = "${self.packages.${system}.mageWrappers}/bin/mage-recreateServer";
        };
        deleteAndRedeployServer = {
          type = "app";
          program = "${self.packages.${system}.mageWrappers}/bin/mage-deleteAndRedeployServer";
        };
        default = self.apps.${system}.mage;
      };

      devShells.${system}.default = pkgs.mkShell {
        buildInputs = [
          pkgs.just
          pkgs.hcloud
          pkgs.kubectl
          pkgs.kubernetes-helm
          pkgs.fluxcd
          pkgs.tailscale
          deploy-rs.packages.${system}.deploy-rs
          nixos-anywhere.packages.${system}.default
          inputs.nixos-facter-modules.packages.${system}.default # facter CLI
          disko.packages.${system}.disko
          pkgs.mage
          pkgs.go
          pkgs.gotools
          pkgs.gopls
          pkgs.sops
        ];
        shellHook = ''
          echo "---"
          echo "NixOS K3s Cluster DevEnv Activated"
          echo "Ensure required environment variables are set (e.g., K3S_CONTROL_PLANE_ADDR, ADMIN_SSH_PUBLIC_KEY)."
          echo "Ensure ./sops.secrets.yaml is created and encrypted."
          echo "Ensure ./machines.nix exists and is populated (this file is gitignored)."
          echo "Ensure Disko layouts exist (e.g., ./disko-configs/hetzner-disko-layout.nix)."
          echo "Ensure ./dummy-hardware-config.nix exists for local pure flake checks."
          echo "Hardware configs for actual deployments will use /etc/nixos/hardware-configuration.nix (Option A)."
          echo "State version is now applied globally to all machines."
          echo "Hostname will be set from the machine name in machines.nix via common.nix."
          echo "---"
        '';
      };
    };
}
</file>

</files>
